# llm-cluster.yaml

apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: llm-inference-cluster # <您的集群名称>，例如: qwen-cluster-apn1
  region: ap-northeast-1       # <您的 AWS 区域>，必须与 S3 区域一致
  version: "1.29"              # 推荐的 EKS 版本

# --- 1. CPU 节点组 (用于 Go 代理和系统组件) ---
nodeGroups:
  - name: cpu-ng
    instanceType: t3.large     # 廉价的 CPU 实例
    desiredCapacity: 2         # 至少两个节点，用于高可用性
    minSize: 1
    maxSize: 3
    # 启用 SSH 访问 (可选)
    ssh:
     publicKeyName: eks-llm-key 

# --- 2. GPU 节点组 (用于 vLLM 服务) ---
  - name: gpu-ng
    instanceType: g4dn.xlarge  # Tesla T4 实例 (您之前确认的选择)
    desiredCapacity: 1         # 至少一个 GPU 节点
    minSize: 0
    maxSize: 2
    amiFamily: AmazonLinux2023
    # 使用深度学习 AMI (确保驱动、CUDA 预装)
    # ami: ami-0f122f8bd80f65978  # 示例 AMI ID，请在您的区域查找 Deep Learning AMI 的最新 ID
    # 必须为 GPU 节点设置标签，以便安装 Device Plugin
    labels: {nvidia.com/gpu: "true"}
    # Taints (Taints): 防止 CPU Pod 跑到 GPU 节点上
    tags:
      "k8s.io/cluster-autoscaler/enabled": "true"
    # EKS 优化 AMI 默认开启，不需要额外配置
